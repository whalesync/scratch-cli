import { Injectable } from '@nestjs/common';
import { WorkbookId } from '@spinner/shared-types';
import { randomUUID } from 'crypto';
import { chunk } from 'lodash';
import { ParsedContent, Schema } from 'src/utils/objects';
import { DbService } from '../db/db.service';
import { WSLogger } from '../logger';
import { BaseJsonTableSpec } from '../remote-service/connectors/types';
import { DIRTY_BRANCH, MAIN_BRANCH, ScratchGitService } from '../scratch-git/scratch-git.service';
import { FileIndexService } from './file-index.service';
import { FileReferenceService } from './file-reference.service';
import { PublishSchemaService } from './publish-schema.service';
import { RefCleanerService } from './ref-cleaner.service';
import { PipelinePhase, PublishPlanInfo, PublishPlanPhase } from './types';
import { parsePath } from './utils';

@Injectable()
export class PublishBuildService {
  constructor(
    private readonly db: DbService,
    private readonly scratchGitService: ScratchGitService,
    private readonly fileIndexService: FileIndexService,
    private readonly fileReferenceService: FileReferenceService,
    private readonly refCleanerService: RefCleanerService,
    private readonly schemaService: PublishSchemaService,
  ) {}

  /**
   * Creates a new pipeline record in the database.
   * Can be called independently (e.g. from controller before enqueuing a job).
   */
  async createPipeline(
    workbookId: string,
    userId: string,
    connectorAccountId?: string,
  ): Promise<{ pipelineId: string; branchName: string }> {
    const pipelineId = randomUUID();
    const branchName = `publish/${userId}/${pipelineId}`;

    await this.db.client.publishPlan.create({
      data: {
        id: pipelineId,
        workbookId,
        userId,
        status: 'planning',
        branchName,
        phases: [],
        connectorAccountId: connectorAccountId || null,
      },
    });

    return { pipelineId, branchName };
  }

  /**
   * Builds the publish pipeline for a given workbook.
   * If pipelineId is provided, uses the existing pipeline record (job flow).
   * Otherwise creates a new one (direct API flow).
   */
  async buildPipeline(
    workbookId: string,
    userId: string,
    connectorAccountId?: string,
    existingPipelineId?: string,
    onProgress?: (step: string) => Promise<void>,
  ): Promise<PublishPlanInfo> {
    let pipelineId: string;
    let branchName: string;

    if (existingPipelineId) {
      // Job flow: pipeline already created by controller
      const existing = await this.db.client.publishPlan.findUnique({ where: { id: existingPipelineId } });
      if (!existing) throw new Error(`Pipeline not found: ${existingPipelineId}`);
      pipelineId = existing.id;
      branchName = existing.branchName;
    } else {
      // Direct flow: create pipeline inline
      const created = await this.createPipeline(workbookId, userId, connectorAccountId);
      pipelineId = created.pipelineId;
      branchName = created.branchName;
    }

    const wkbId = workbookId as WorkbookId;

    // 2. Get diff between main and dirty
    let changes = (await this.scratchGitService.getRepoStatus(wkbId)) as Array<{
      path: string;
      status: 'added' | 'modified' | 'deleted';
    }>;

    await onProgress?.(`Diffing branches (${changes.length} changes found)`);

    if (connectorAccountId) {
      // Find all data folders for this connector in this workbook
      const dataFolders = await this.db.client.dataFolder.findMany({
        where: { workbookId: wkbId, connectorAccountId },
      });

      const prefixes = dataFolders
        .map((df) => df.path)
        .filter((p): p is string => !!p)
        .map((p) => (p.startsWith('/') ? p.substring(1) : p)) // Normalize: remove leading slash
        .map((p) => (p.endsWith('/') ? p : p + '/'));

      if (prefixes.length > 0) {
        changes = changes.filter((c) => prefixes.some((prefix) => c.path.startsWith(prefix)));
      } else {
        // No folders? Then no changes for this connector.
        changes = [];
      }
    }

    const modifiedFiles = changes.filter((c) => c.status === 'modified');
    const addedFiles = changes.filter((c) => c.status === 'added');
    const deletedFiles = changes.filter((c) => c.status === 'deleted');

    const phases: PipelinePhase[] = [];

    // Cache table specs to avoid repeated reads
    // Cache table specs to avoid repeated reads
    const dataFolderCache = new Map<string, { id: string; spec: BaseJsonTableSpec } | null>();

    const getDataFolderInfo = async (folderPath: string) => {
      return this.schemaService.getDataFolderInfo(workbookId, folderPath, dataFolderCache);
    };

    const addedPathsSet = new Set(addedFiles.map((f) => f.path));
    const deletedPathsSet = new Set(deletedFiles.map((f) => f.path));

    // --- Prepare for "Delete Ref Clearing" ---
    // 1. Identify Deleted Record IDs

    const deletedRecordIds = new Set<string>();
    const deletedFileRecordIds = new Map<string, string | null>();
    const targetsForRefCheck: Array<{ folderPath: string; fileName: string; recordId?: string }> = [];
    const deletedTotal = deletedFiles.length;
    let deletedProcessed = 0;

    for (const del of deletedFiles) {
      deletedProcessed++;
      if (deletedProcessed === 1 || deletedProcessed % 50 === 0 || deletedProcessed === deletedTotal) {
        await onProgress?.(`Resolving deleted record IDs (${deletedProcessed}/${deletedTotal})`);
      }
      const { folderPath, filename: fileName } = parsePath(del.path);
      const recordId = await this.fileIndexService.getRecordId(workbookId, folderPath, fileName);

      if (recordId) deletedRecordIds.add(recordId);
      deletedFileRecordIds.set(del.path, recordId || null);
      targetsForRefCheck.push({ folderPath, fileName, recordId: recordId || undefined });
    }

    // 2. Identify Inbound Refs to Deleted Files
    // TODO: do we need to search in both branches?
    const searchBranches = [MAIN_BRANCH, DIRTY_BRANCH];
    const inboundRefs = await this.fileReferenceService.findRefsToFiles(
      workbookId,
      targetsForRefCheck,
      searchBranches,
      onProgress,
    );

    // Identify files that need editing because they reference a deleted file
    const filesReferringToDeletedFiles = new Set<string>();
    for (const ref of inboundRefs) {
      // Exclude files that are themselves being deleted
      if (!deletedPathsSet.has(ref.sourceFilePath)) {
        filesReferringToDeletedFiles.add(ref.sourceFilePath);
      }
    }

    // --- Phase 1: [edit] ---
    // Process Union of Modified Files and Ref-Clearing Candidate Files
    const filesToProcessInEditPhase = new Set(modifiedFiles.map((f) => f.path));
    for (const p of filesReferringToDeletedFiles) filesToProcessInEditPhase.add(p);

    let editCount = 0;
    const editPhaseTotal = filesToProcessInEditPhase.size;
    let editPhaseProcessed = 0;
    const planEntries: Array<{
      filePath: string;
      phase: PublishPlanPhase;
      operation: ParsedContent;
      remoteRecordId?: string | null;
      dataFolderId?: string | null;
      status: string;
    }> = [];

    for (const filePath of filesToProcessInEditPhase) {
      editPhaseProcessed++;
      if (editPhaseProcessed === 1 || editPhaseProcessed % 50 === 0 || editPhaseProcessed === editPhaseTotal) {
        await onProgress?.(`Processing edits (${editPhaseProcessed}/${editPhaseTotal})`);
      }
      WSLogger.info({
        source: 'PublishBuildService.buildPipeline',
        message: `Processing file in edit phase: ${filePath}`,
        workbookId,
      });
      // Fetch content: try Dirty first, fallback to main
      // Fallback is needed since we might need to clear refs from files that are deleted in dirty.
      let fileData = await this.scratchGitService.getRepoFile(wkbId, 'dirty', filePath);
      if (!fileData) {
        WSLogger.warn({
          source: 'PublishBuildService.buildPipeline',
          message: `File not found in dirty branch, falling back to main: ${filePath}`,
          workbookId,
        });
        fileData = await this.scratchGitService.getRepoFile(wkbId, 'main', filePath);
      }

      if (fileData?.content) {
        let contentObj: ParsedContent;
        try {
          contentObj = JSON.parse(fileData.content) as ParsedContent;
        } catch {
          // Not JSON? Just commit as is if it was user-modified.
          if (filesToProcessInEditPhase.has(filePath) && modifiedFiles.some((m) => m.path === filePath)) {
            const { folderPath } = parsePath(filePath);
            const info = await getDataFolderInfo(folderPath);

            planEntries.push({
              filePath,
              phase: 'edit',
              operation: JSON.parse(fileData.content) as ParsedContent,
              dataFolderId: info?.id,
              status: 'pending',
            });
            editCount++;
          }
          continue;
        }

        const { folderPath } = parsePath(filePath);
        const info = await getDataFolderInfo(folderPath);
        const schema = info?.spec?.schema as Schema;
        const dataFolderId = info?.id;

        // --- TWO PASS STRIPPING ---

        // Pass 1: Strip references to DELETED records.
        const pass1ContentObj = await this.refCleanerService.stripReferencesWithSchema(
          workbookId,
          contentObj,
          schema,
          deletedPathsSet,
          deletedRecordIds,
          'IDS_ONLY',
        );
        const pass1ContentStr = JSON.stringify(pass1ContentObj, null, 2);

        // Pass 2: Strip references to NEW records (Pseudo-refs).
        const pass2ContentObj = await this.refCleanerService.stripReferencesWithSchema(
          workbookId,
          pass1ContentObj,
          schema,
          addedPathsSet,
          undefined,
          'PSEUDO_ONLY',
        );
        const pass2ContentStr = JSON.stringify(pass2ContentObj, null, 2);

        // Determine Edit Operation
        const originalContentStr = JSON.stringify(contentObj, null, 2);
        const isUserModified = modifiedFiles.some((m) => m.path === filePath);
        const isRefCleared = pass1ContentStr !== originalContentStr;
        const isPseudoStripped = pass2ContentStr !== pass1ContentStr;

        if (isUserModified || isRefCleared || isPseudoStripped) {
          planEntries.push({
            filePath,
            phase: 'edit',
            operation: pass2ContentObj,
            dataFolderId: dataFolderId || null,
            status: 'pending',
          });
          editCount++;

          // Backfill Logic
          if (pass2ContentStr !== pass1ContentStr) {
            planEntries.push({
              filePath,
              phase: 'backfill',
              operation: pass1ContentObj,
              dataFolderId: dataFolderId || null,
              status: 'pending',
            });
          }
        }
      }
    }

    if (editCount > 0) {
      phases.push({ type: 'edit', recordCount: editCount });
    }

    // --- Phase 2: [create] ---
    let createCount = 0;
    let createPhaseProcessed = 0;
    const createPhaseTotal = addedFiles.length;

    for (const add of addedFiles) {
      createPhaseProcessed++;
      if (createPhaseProcessed === 1 || createPhaseProcessed % 50 === 0 || createPhaseProcessed === createPhaseTotal) {
        await onProgress?.(`Processing creates (${createPhaseProcessed}/${createPhaseTotal})`);
      }
      const fileData = await this.scratchGitService.getRepoFile(wkbId, 'dirty', add.path);
      if (fileData?.content) {
        const { folderPath } = parsePath(add.path);
        const info = await getDataFolderInfo(folderPath);

        let contentObj: ParsedContent;
        try {
          contentObj = JSON.parse(fileData.content) as ParsedContent;
        } catch {
          planEntries.push({
            filePath: add.path,
            phase: 'create',
            operation: JSON.parse(fileData.content) as ParsedContent,
            dataFolderId: info?.id || null,
            status: 'pending',
          });
          createCount++;
          continue;
        }

        const schema = info?.spec?.schema as Schema;
        const dataFolderId = info?.id;

        // Pass 1: Strip Deleted
        const pass1ContentObj = await this.refCleanerService.stripReferencesWithSchema(
          workbookId,
          contentObj,
          schema,
          deletedPathsSet,
        );
        const pass1ContentStr = JSON.stringify(pass1ContentObj, null, 2);

        // Pass 2: Strip Pseudo
        const pass2ContentObj = await this.refCleanerService.stripReferencesWithSchema(
          workbookId,
          pass1ContentObj,
          schema,
          addedPathsSet,
          undefined,
          'PSEUDO_ONLY',
        );
        const pass2ContentStr = JSON.stringify(pass2ContentObj, null, 2);

        planEntries.push({
          filePath: add.path,
          phase: 'create',
          operation: pass2ContentObj,
          dataFolderId: dataFolderId || null,
          status: 'pending',
        });
        createCount++;

        if (pass2ContentStr !== pass1ContentStr) {
          planEntries.push({
            filePath: add.path,
            phase: 'backfill',
            operation: pass1ContentObj,
            dataFolderId: dataFolderId || null,
            status: 'pending',
          });
        }
      }
    }

    if (createCount > 0) {
      phases.push({ type: 'create', recordCount: createCount });
    }

    // --- Phase 3: [delete] ---
    const deletePhaseTotal = deletedFiles.length;
    let deletePhaseProcessed = 0;

    for (const del of deletedFiles) {
      deletePhaseProcessed++;
      if (deletePhaseProcessed === 1 || deletePhaseProcessed % 50 === 0 || deletePhaseProcessed === deletePhaseTotal) {
        await onProgress?.(`Processing deletes (${deletePhaseProcessed}/${deletePhaseTotal})`);
      }
      // recordId was already looked up above when building deletedRecordIds
      const recordId = deletedFileRecordIds.get(del.path);
      const { folderPath } = parsePath(del.path);
      const info = await getDataFolderInfo(folderPath);

      planEntries.push({
        filePath: del.path,
        phase: 'delete',
        operation: {},
        remoteRecordId: recordId || null,
        dataFolderId: info?.id || null,
        status: 'pending',
      });
    }

    if (deletedFiles.length > 0) {
      phases.push({ type: 'delete', recordCount: deletedFiles.length });
    }

    // Create Entries
    if (planEntries.length > 0) {
      await onProgress?.(`Saving plan entries (${planEntries.length} entries)`);
      const chunks = chunk(planEntries, 2000);
      for (const c of chunks) {
        await this.db.client.publishPlanEntry.createMany({
          data: c.map((e) => ({
            planId: pipelineId,
            filePath: e.filePath,
            phase: e.phase,
            operation: e.operation,
            remoteRecordId: e.remoteRecordId ?? null,
            dataFolderId: e.dataFolderId ?? null,
            status: e.status,
          })),
        });
      }
    }

    // Mark as planned (ready to run)
    await this.db.client.publishPlan.update({
      where: { id: pipelineId },
      data: { status: 'planned' },
    });

    return {
      pipelineId,
      workbookId,
      userId,
      phases,
      branchName,
      createdAt: new Date(),
      status: 'planned',
    };
  }
}
